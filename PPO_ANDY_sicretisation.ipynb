{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check and use GPU if available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch.distributions import MultivariateNormal, Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int, \n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(ContinuousActor, self).__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(in_dim, 32)\n",
    "        self.mu_layer = nn.Linear(32, out_dim)\n",
    "        self.log_std_layer = nn.Linear(32, out_dim)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden(state))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.tanh(self.log_std_layer(x))\n",
    "\n",
    "        std = torch.exp(log_std)\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE from another notebook\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        self.output_activation = activation\n",
    "\n",
    "        # Initialization using Xavier normal (a popular technique for initializing weights in NNs)\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        # flatten the input\n",
    "        # x = torch.flatten(x)\n",
    "        # print(x)\n",
    "        # # normalize the inputs for cartopole-v1\n",
    "        # x[0] = x[0] / 4.8\n",
    "        # x[2] = x[2] / 0.418\n",
    "        # First normalize the inputs\n",
    "        #x = (x - x.mean()) / x.std()\n",
    "\n",
    "        # Then pass through the layers\n",
    "        x = self.activation1(self.linear1(x))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# HERE ADDED ns, nA and discretized_continuous_action_space and actions_transform=None********************************************************************************\n",
    "def generate_single_episode(env, policy_net, nS, nA, discretized_continuous_action_space, actions_transform=None):\n",
    "    \"\"\"\n",
    "    Generates an episode by executing the current policy in the given env\n",
    "    \"\"\"\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    max_t = 1000 # max horizon within one episode\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box): # continuous action space\n",
    "        max_possible_action = float(env.action_space.high[0]) # Only works with a action space dim of 1\n",
    "        min_possible_action = float(env.action_space.low[0]) # Only works with a action space dim of 1\n",
    "\n",
    "    for t in range(max_t):\n",
    "        #print(t)\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space: # HERE ADDED dicretized_continuous_action_space***********\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        else:\n",
    "            state = torch.from_numpy(state.flatten()).float()\n",
    "            \n",
    "\n",
    "        # if action space is discrete or continuous\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space: # HERE ADDED dicretized_continuous_action_space***********\n",
    "            # normalize the state\n",
    "            probs = policy_net.forward(Variable(state)) # get each action choice probability with the current policy network\n",
    "            action = np.random.choice(nA, p=np.squeeze(probs.detach().numpy())) # probablistic ## HERE ADDED nA instead of env.action_space.n******************\n",
    "        else:\n",
    "            action, dist = policy_net.forward(state) # continuous\n",
    "            # clip action to the action space\n",
    "            action = torch.clamp(action, min=min_possible_action, max=max_possible_action)\n",
    "            action = action.detach().numpy()\n",
    "            probs = dist\n",
    "\n",
    "\n",
    "        # compute the log_prob to use this in parameter update\n",
    "        log_prob = None\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space: # HERE ADDED dicretized_continuous_action_space***********\n",
    "            log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        else:\n",
    "            log_prob = dist.log_prob(torch.tensor(action))\n",
    "            # print(f\"log_prob: {log_prob}\")\n",
    "            \n",
    "        #print(log_prob)\n",
    "        \n",
    "        # append values\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # take a selected action\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            #reward = (reward + 8.1) / 8.1 # normalize reward only for pendulum\n",
    "        # HERE ********************************************************************************************************************\n",
    "        elif discretized_continuous_action_space:\n",
    "            assert actions_transform is not None, \"actions_transform must be provided for discretized_continuous_action_space\" \n",
    "            state, reward, terminated, truncated, _ = env.step([actions_transform[action]])\n",
    "            reward = (reward + 8.1) / 8.1 # normalize reward only for pendulum\n",
    "        #**************************************************************************************************************************\n",
    "        else:\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            reward = (reward + 8.1) / 8.1 # normalize reward only for pendulum\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated | truncated:\n",
    "            break\n",
    "\n",
    "    # print(f\"Episode length: {t}\")\n",
    "    # print(f\"Episode reward: {np.sum(rewards)}\")\n",
    "    # print(f\"Episode states: {states}\")\n",
    "    # print(f\"Episode actions: {actions}\")\n",
    "    # print(f\"Episode rewards: {rewards}\")\n",
    "    # print(f\"Episode log_probs: {log_probs}\")\n",
    "    # print(f\"Episode state\", state)\n",
    "    # print(f\"Episode action trans\", action)\n",
    "    # print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "# HERE ADDED ns, nA and discretized_continuous_action_space and actions_transform=None********************************************************************************\n",
    "def evaluate_policy(env, policy_net, nS, nA, discretized_continuous_action_space, actions_transform=None):\n",
    "    \"\"\"\n",
    "    Compute accumulative trajectory reward\n",
    "    \"\"\"\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net, nS, nA, discretized_continuous_action_space, actions_transform) # HERE ADDED ns, nA and discretized_continuous_action_space and actions_transform************\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE ADDED nS, nA and discretized_continuous_action_space and actions_transform=None*********************************************************************************\n",
    "def train_PPO(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, nS, nA, discretized_continuous_action_space, actions_transform = None, clip_val=0.2, gamma=0.99, entropy_coef=0.005, lamda=0.95):\n",
    "    \"\"\"\n",
    "    Trains the policy network using PPO\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate an episode with the current policy network\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net, nS, nA, discretized_continuous_action_space, actions_transform)\n",
    "    T = len(states)\n",
    "    \n",
    "    # Create tensors depending on if it is discrete or continuous action space\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space:\n",
    "        actions = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    else:\n",
    "        actions = torch.FloatTensor(actions).to(device).view(-1,1)\n",
    "\n",
    "\n",
    "    states = np.vstack(states).astype(float)\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device).view(-1,1)\n",
    "    log_probs = torch.FloatTensor(log_probs).to(device).view(-1,1)\n",
    "\n",
    "    # Compute the generalized advantage estimate\n",
    "    Gs = []\n",
    "    G = 0\n",
    "    for t in range(T-1,-1,-1):\n",
    "        delta = (rewards[t] + gamma*value_net(states[t]) - value_net(states[t-1]))\n",
    "        G = delta + gamma * G * lamda\n",
    "        Gs.insert(0,G)\n",
    "    Gs = torch.tensor(Gs).view(-1,1)\n",
    "    #print(f\"Gs: {Gs}\")\n",
    "    \n",
    "    # for t in range(T-1,-1,-1): # iterate in backward order to make the computation easier\n",
    "    #     G = rewards[t] + gamma*G\n",
    "    #     Gs.insert(0,G)\n",
    "    # Gs = torch.tensor(Gs).view(-1,1)\n",
    "    \n",
    "    # Compute the advantage\n",
    "    state_vals = value_net(states).to(device)\n",
    "    #print(f\"State values: {state_vals}\")\n",
    "    with torch.no_grad():\n",
    "        A_k = Gs - state_vals\n",
    "        \n",
    "    for t in range(num_epochs):\n",
    "        # Compute the value of the current states\n",
    "        V = value_net(states).to(device)\n",
    "        #print(f\"Value: {V}\")\n",
    "\n",
    "        # Calculate probability of each action under the updated policy\n",
    "        # compute the log_prob to use it in parameter update\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space: # HERE ADDED dicretized_continuous_action_space***********\n",
    "            probs = policy_net.forward(states).to(device)\n",
    "            curr_log_probs = torch.log(torch.gather(probs, 1, actions))\n",
    "            # print(f\"probs, discrete: {probs}\")\n",
    "        else:\n",
    "            _, probs = policy_net.forward(states)\n",
    "            curr_log_probs = probs.log_prob(actions)\n",
    "            # print(f\"probs, continuous: {probs}\")\n",
    "\n",
    "        # Calculate ratios r(theta)\n",
    "        ratios = torch.exp(curr_log_probs - log_probs)\n",
    "        \n",
    "        # Calculate two surrogate loss terms in cliped loss\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1-clip_val, 1+clip_val) * A_k\n",
    "        \n",
    "        # entropy \n",
    "        # HERE ADDED dicretized_continuous_action_space AND reverse the not*************************************************************************************\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space: \n",
    "            # Calculate clipped loss value\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean() # Need negative sign to run Gradient Ascent\n",
    "        else:\n",
    "            entropy = probs.entropy().mean()\n",
    "            actor_loss = (-torch.min(surr1, surr2).mean() - entropy_coef * entropy)\n",
    "        #****************************************************************************************************************************************************\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Update value net\n",
    "        critic_loss = nn.MSELoss()(V, Gs)\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "    # print(f\"Value: {V}\")\n",
    "    # print(f\"Advantage: {A_k}\")\n",
    "    # print(f\"Ratios: {ratios}\")\n",
    "    # print(f\"Surrogate 1: {surr1}\")\n",
    "    # print(f\"Surrogate 2: {surr2}\")\n",
    "    # print(f\"Actor loss: {actor_loss}\")\n",
    "    # print(f\"Critic loss: {critic_loss}\")\n",
    "    # print(\"-----------------------------------------------------------------------------\")\n",
    "        \n",
    "    return policy_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The avg. test reward for episode 0 is 53.81091809916482 with std of 30.181170769555386.\n",
      "Episode: 50\n",
      "The avg. test reward for episode 50 is 74.12877525171115 with std of 21.141939191265767.\n",
      "Episode: 100\n",
      "The avg. test reward for episode 100 is 75.90052526058597 with std of 26.32409788729959.\n",
      "Episode: 150\n",
      "The avg. test reward for episode 150 is 63.15925059535142 with std of 23.19549217105326.\n",
      "Episode: 200\n",
      "The avg. test reward for episode 200 is 43.31209688163818 with std of 35.34744831188236.\n",
      "Episode: 250\n",
      "The avg. test reward for episode 250 is 54.717379749439985 with std of 35.136319080713484.\n",
      "Episode: 300\n",
      "The avg. test reward for episode 300 is 65.02027820257841 with std of 25.903730024042247.\n",
      "Episode: 350\n",
      "The avg. test reward for episode 350 is 47.89018259343614 with std of 31.094756883753913.\n",
      "Episode: 400\n",
      "The avg. test reward for episode 400 is 68.4308915264641 with std of 25.69573958746214.\n",
      "Episode: 450\n",
      "The avg. test reward for episode 450 is 76.65590252053882 with std of 16.20623929857537.\n",
      "Episode: 500\n",
      "The avg. test reward for episode 500 is 73.2590447037865 with std of 28.169015883080935.\n",
      "Episode: 550\n",
      "The avg. test reward for episode 550 is 67.82182168882392 with std of 19.955736559942196.\n",
      "Episode: 600\n",
      "The avg. test reward for episode 600 is 76.36584836663795 with std of 27.433239438771114.\n",
      "Episode: 650\n",
      "The avg. test reward for episode 650 is 63.94176533855025 with std of 22.648566728140338.\n",
      "Episode: 700\n",
      "The avg. test reward for episode 700 is 68.09111362938769 with std of 21.332472232055345.\n",
      "Episode: 750\n",
      "The avg. test reward for episode 750 is 83.27452154741437 with std of 17.859505135526174.\n",
      "Episode: 800\n",
      "The avg. test reward for episode 800 is 80.70267442303087 with std of 16.622015965417145.\n",
      "Episode: 850\n",
      "The avg. test reward for episode 850 is 85.6594023245524 with std of 11.355434392845742.\n",
      "Episode: 900\n",
      "The avg. test reward for episode 900 is 94.50486940665601 with std of 12.750959510317157.\n",
      "Episode: 950\n",
      "The avg. test reward for episode 950 is 90.0581703671093 with std of 15.964296438480357.\n",
      "Episode: 1000\n",
      "The avg. test reward for episode 1000 is 77.60055470609345 with std of 11.0285323591272.\n",
      "Episode: 1050\n",
      "The avg. test reward for episode 1050 is 96.11301977981408 with std of 18.420776388892474.\n",
      "Episode: 1100\n",
      "The avg. test reward for episode 1100 is 100.00817294320925 with std of 20.97932886980743.\n",
      "Episode: 1150\n",
      "The avg. test reward for episode 1150 is 115.89571512244181 with std of 17.702133296466478.\n",
      "Episode: 1200\n",
      "The avg. test reward for episode 1200 is 114.19496674264842 with std of 15.585352606065271.\n",
      "Episode: 1250\n",
      "The avg. test reward for episode 1250 is 127.69455256134015 with std of 17.71705522463783.\n",
      "Episode: 1300\n",
      "The avg. test reward for episode 1300 is 123.3135038828317 with std of 28.23908847092339.\n",
      "Episode: 1350\n",
      "The avg. test reward for episode 1350 is 153.34015736182317 with std of 19.107908496278526.\n",
      "Episode: 1400\n",
      "The avg. test reward for episode 1400 is 153.7603193829025 with std of 27.41391276454553.\n",
      "Episode: 1450\n",
      "The avg. test reward for episode 1450 is 156.75486528537576 with std of 31.352437789085176.\n",
      "Episode: 1500\n",
      "The avg. test reward for episode 1500 is 164.79957110391067 with std of 23.99498365322609.\n",
      "Episode: 1550\n",
      "The avg. test reward for episode 1550 is 143.15429818179615 with std of 23.275986703211004.\n",
      "Episode: 1600\n",
      "The avg. test reward for episode 1600 is 173.82919400524875 with std of 19.080670304295804.\n",
      "Episode: 1650\n",
      "The avg. test reward for episode 1650 is 156.48531685673456 with std of 22.554331472896667.\n",
      "Episode: 1700\n",
      "The avg. test reward for episode 1700 is 164.3013632668811 with std of 21.841574810881628.\n",
      "Episode: 1750\n",
      "The avg. test reward for episode 1750 is 167.99788302158748 with std of 21.525371988549587.\n",
      "Episode: 1800\n",
      "The avg. test reward for episode 1800 is 170.71899772559536 with std of 24.525297991300985.\n",
      "Episode: 1850\n",
      "The avg. test reward for episode 1850 is 161.62875844384325 with std of 18.8769884069988.\n",
      "Episode: 1900\n",
      "The avg. test reward for episode 1900 is 176.18824440136706 with std of 17.45008956664429.\n",
      "Episode: 1950\n",
      "The avg. test reward for episode 1950 is 170.29755760697367 with std of 17.1328613146344.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [10:20<20:41, 620.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The avg. test reward for episode 0 is 54.785632885079146 with std of 32.05609890712485.\n",
      "Episode: 50\n",
      "The avg. test reward for episode 50 is 51.289915256339135 with std of 34.36918752239968.\n",
      "Episode: 100\n",
      "The avg. test reward for episode 100 is 59.0437434404296 with std of 29.405286116809783.\n",
      "Episode: 150\n",
      "The avg. test reward for episode 150 is 70.43010284691265 with std of 23.28484947120025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [11:23<22:46, 683.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m value_net_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbaseline_lr)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_ite):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Train networks with PPO # HERE ADDED nA, nS and discretized_continuous_action_space******************************************************************\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     policy_net, value_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_PPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscretized_continuous_action_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentropy_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m#********************************************************************************************************************************************************\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mtrain_PPO\u001b[1;34m(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, nS, nA, discretized_continuous_action_space, actions_transform, clip_val, gamma, entropy_coef, lamda)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mTrains the policy network using PPO\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate an episode with the current policy network\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m states, actions, rewards, log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_single_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscretized_continuous_action_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(states)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Create tensors depending on if it is discrete or continuous action space\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 70\u001b[0m, in \u001b[0;36mgenerate_single_episode\u001b[1;34m(env, policy_net, nS, nA, discretized_continuous_action_space, actions_transform)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete) \u001b[38;5;129;01mor\u001b[39;00m discretized_continuous_action_space: \u001b[38;5;66;03m# HERE ADDED dicretized_continuous_action_space***********\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# normalize the state\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     probs \u001b[38;5;241m=\u001b[39m policy_net\u001b[38;5;241m.\u001b[39mforward(Variable(state)) \u001b[38;5;66;03m# get each action choice probability with the current policy network\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(nA, p\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())) \u001b[38;5;66;03m# probablistic ## HERE ADDED nA instead of env.action_space.n******************\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     action, dist \u001b[38;5;241m=\u001b[39m policy_net\u001b[38;5;241m.\u001b[39mforward(state) \u001b[38;5;66;03m# continuous\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define parameter values\n",
    "#env_name = 'CartPole-v1' # environment name\n",
    "env_name = 'Pendulum-v1' # environment name\n",
    "\n",
    "#env_name = 'LunarLander-v2' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "\n",
    "\n",
    "num_train_ite = 2000 # number of training iterations\n",
    "num_seeds = 1 # fit model with 3 different seeds and plot average performance of 3 seeds\n",
    "num_epochs = 10 # how many times we iterate the entire training dataset passing through the training\n",
    "eval_freq = 50 # run evaluation of policy at each eval_freq trials\n",
    "eval_epi_index = num_train_ite//eval_freq # use to create x label for plot\n",
    "returns = np.zeros((num_seeds, eval_epi_index))\n",
    "gamma = 0.99 # discount factor\n",
    "clip_val = 0.2 # hyperparameter epsilon in clip objective\n",
    "entropy_coef = 0.001 # hyperparameter entropy coefficient (0.005)\n",
    "\n",
    "# Create the environment.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "\n",
    "\n",
    "# Discretized Action Space**********************************************************************************************\n",
    "discretized_continuous_action_space = False\n",
    "\n",
    "if env_name == 'Pendulum-v1':\n",
    "    discretized_continuous_action_space = True\n",
    "    \n",
    "actions_transform = None\n",
    "if discretized_continuous_action_space:\n",
    "    action_range = (env.action_space.low[0], env.action_space.high[0])\n",
    "    n_actions = 11\n",
    "    actions_transform = np.linspace(action_range[0], action_range[1], n_actions)\n",
    "\n",
    "#************************************************************************************************************************\n",
    "\n",
    "#detect if the environment is discrete or continuous\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nA = env.action_space.n\n",
    "else:\n",
    "    # HERE ADDED dicretized_continuous_action_space********************************************************************\n",
    "    if discretized_continuous_action_space:\n",
    "        nA = n_actions\n",
    "    else:\n",
    "        nA = env.action_space.shape[0]\n",
    "    #****************************************************************************************************************\n",
    "\n",
    "# detect if the environment state is discrete or continuous\n",
    "if isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.n\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "\n",
    "policy_lr = 1e-4 # policy network's learning rate \n",
    "baseline_lr = 5e-4 # value network's learning rate\n",
    "\n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # Define policy and value networks\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete) or discretized_continuous_action_space: # HERE ADDED dicretized_continuous_action_space***********\n",
    "        policy_net = NeuralNet(nS, nA, torch.nn.Softmax()) # Normally this one works with CartPole-v0\n",
    "    else:\n",
    "        policy_net = ContinuousActor(nS, nA)\n",
    "    policy_net_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "    value_net = NeuralNet(nS, 1, torch.nn.ReLU())\n",
    "    value_net_optimizer = optim.Adam(value_net.parameters(), lr=baseline_lr)\n",
    "    \n",
    "    for m in range(num_train_ite):\n",
    "        # Train networks with PPO # HERE ADDED nA, nS and discretized_continuous_action_space******************************************************************\n",
    "        policy_net, value_net = train_PPO(\n",
    "            env, policy_net, policy_net_optimizer, value_net, value_net_optimizer, num_epochs, \n",
    "            nS, nA, discretized_continuous_action_space, actions_transform=actions_transform,\n",
    "            clip_val=clip_val, gamma=gamma, entropy_coef=entropy_coef)\n",
    "        #********************************************************************************************************************************************************\n",
    "        if m % eval_freq == 0:\n",
    "            print(\"Episode: {}\".format(m))\n",
    "            G = np.zeros(20)\n",
    "            for k in range(20):\n",
    "                g = evaluate_policy(env, policy_net, nS, nA, discretized_continuous_action_space, actions_transform) # HERE ADDED nS, nA and discretized_continuous_action_space and actions_transform**************\n",
    "                G[k] = g\n",
    "\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(\"The avg. test reward for episode {0} is {1} with std of {2}.\".format(m, reward_mean, reward_sd))\n",
    "            reward_means.append(reward_mean)\n",
    "    returns[i] = np.array(reward_means)\n",
    "\n",
    "# save the policy network\n",
    "torch.save(policy_net.state_dict(), f\"policy_net_{env_name}.pt\")\n",
    "\n",
    "\n",
    "# Plot the performance over iterations\n",
    "x = np.arange(eval_epi_index)*eval_freq\n",
    "avg_returns = np.mean(returns, axis=0)\n",
    "max_returns = np.max(returns, axis=0)\n",
    "min_returns = np.min(returns, axis=0)\n",
    "\n",
    "plt.fill_between(x, min_returns, max_returns, alpha=0.1)\n",
    "plt.plot(x, avg_returns, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "plt.title(\"PPO Learning Curve\", fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19139662, 0.9815128 , 5.5108323 ], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the policy network\n",
    "torch.save(policy_net.state_dict(), f\"policy_net_discrete_working{env_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_10680\\1596193349.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probs = policy_net.forward(torch.tensor(state, dtype=torch.float32))#************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -906.0140322749731\n"
     ]
    }
   ],
   "source": [
    "# FOR  DISCETIZED CONTINUOUS ACTION SPACE****************************************************************************************\n",
    "# THINGS HAVE CHANGED HERE TO DO IT FOR DISCRETIZED CONTINUOUS ACTION SPACE\n",
    "\n",
    "import time\n",
    "#create an environment to test and visualize the policy\n",
    "#env_name = 'CartPole-v0' # environment name\n",
    "env_name = 'Pendulum-v1' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.n\n",
    "    policy_net = NeuralNet(nS, nA, torch.nn.Softmax())\n",
    "elif discretized_continuous_action_space: #************************************************************************************\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = n_actions\n",
    "    policy_net = NeuralNet(nS, nA, torch.nn.Softmax())\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.shape[0]\n",
    "    policy_net = ContinuousActor(nS, nA)\n",
    "\n",
    "# load the policy network\n",
    "policy_net.load_state_dict(torch.load(f\"policy_net_{env_name}.pt\"))\n",
    "# policy_net.eval()\n",
    "\n",
    "episodes = 1\n",
    "state = env.reset()[0]\n",
    "rewards = []\n",
    "for i in range(episodes):\n",
    "    while True:\n",
    "        probs = policy_net.forward(torch.tensor(state, dtype=torch.float32))#************************************************************************************\n",
    "        action = np.random.choice(nA, p=np.squeeze(probs.detach().numpy()))#************************************************************************************\n",
    "        state, reward, done, truncated, _ = env.step([actions_transform[action]]) #************************************************************************************\n",
    "        if done or truncated:\n",
    "            break\n",
    "        state = torch.from_numpy(state.flatten()).float()\n",
    "        rewards.append(reward)\n",
    "    print(f\"Episode {i}: {np.sum(rewards)}\")\n",
    "    env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_13460\\2025769064.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action, _ = policy_net.forward(torch.tensor(state, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -1347.13148178803\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#create an environment to test and visualize the policy\n",
    "#env_name = 'CartPole-v0' # environment name\n",
    "#env_name = 'Pendulum-v1' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.n\n",
    "    policy_net = NeuralNet(nS, nA, torch.nn.Softmax())\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.shape[0]\n",
    "    policy_net = ContinuousActor(nS, nA)\n",
    "\n",
    "# load the policy network\n",
    "policy_net.load_state_dict(torch.load(f\"policy_net_{env_name}.pt\"))\n",
    "# policy_net.eval()\n",
    "\n",
    "episodes = 1\n",
    "state = env.reset()[0]\n",
    "rewards = []\n",
    "for i in range(episodes):\n",
    "    while True:\n",
    "        action, _ = policy_net.forward(torch.tensor(state, dtype=torch.float32))\n",
    "        # clip action to the action space\n",
    "        action = torch.clamp(action, min=-2, max=2)\n",
    "        \n",
    "        # take a selected action\n",
    "        action = action.detach().numpy()\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "        state = torch.from_numpy(state.flatten()).float()\n",
    "        rewards.append(reward)\n",
    "    print(f\"Episode {i}: {np.sum(rewards)}\")\n",
    "    env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1252830459db436a85b9800859f42f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb26d51d2d4e4bfcbb78ebc6d3d05f6b",
       "IPY_MODEL_248b8eb6d6d246e5b108afa59b6c53af"
      ],
      "layout": "IPY_MODEL_85ea1f2de43740659776d173f81b9e11"
     }
    },
    "171dadde77184657add3f9136b23b00e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "248b8eb6d6d246e5b108afa59b6c53af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3d9e9d1989d40438c07c1b652d747dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a0c39bd339746f794a67281a2442ad2",
      "value": 1
     }
    },
    "456be98214854aa2b6c3ac281a078ad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78c135b964044bcf841f6bb944e01b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9392daa2b25d4a27b681b7c836bdbf4a",
      "placeholder": "​",
      "style": "IPY_MODEL_456be98214854aa2b6c3ac281a078ad4",
      "value": "0.066 MB of 0.066 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "7a0c39bd339746f794a67281a2442ad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c4c2ae0c4454b07b3a5d63dca8d795c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85ea1f2de43740659776d173f81b9e11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8627cad6d7864563a89132995c034182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c4a0434c7a7413289ff097106d7318c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3a393a56c20445d848adc997aa8018e",
      "value": 1
     }
    },
    "9392daa2b25d4a27b681b7c836bdbf4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c4a0434c7a7413289ff097106d7318c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaecdcf1288446f9b7cd84f4f3044de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78c135b964044bcf841f6bb944e01b06",
       "IPY_MODEL_8627cad6d7864563a89132995c034182"
      ],
      "layout": "IPY_MODEL_171dadde77184657add3f9136b23b00e"
     }
    },
    "bb26d51d2d4e4bfcbb78ebc6d3d05f6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c4c2ae0c4454b07b3a5d63dca8d795c",
      "placeholder": "​",
      "style": "IPY_MODEL_c657703d2bce404d87e4dd41a28222d4",
      "value": "0.019 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "c657703d2bce404d87e4dd41a28222d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3a393a56c20445d848adc997aa8018e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3d9e9d1989d40438c07c1b652d747dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
