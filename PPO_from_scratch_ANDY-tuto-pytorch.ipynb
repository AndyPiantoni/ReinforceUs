{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check and use GPU if available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousPolicyNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        # Output layer for mean\n",
    "        self.mean_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        \n",
    "        # Output layer for log(std_dev), which is more numerically stable\n",
    "        self.log_stddev_layer = torch.nn.Linear(layers[2], output_size)\n",
    "\n",
    "        # Initialization using Xavier normal\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "\n",
    "        torch.nn.init.xavier_normal_(self.mean_layer.weight)\n",
    "        torch.nn.init.xavier_normal_(self.log_stddev_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        \n",
    "        mean = self.mean_layer(x)\n",
    "        log_stddev = self.log_stddev_layer(x)\n",
    "        return mean, log_stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE from another notebook\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        self.output_activation = activation\n",
    "\n",
    "        # Initialization using Xavier normal (a popular technique for initializing weights in NNs)\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_single_episode(env, policy_net):\n",
    "    \"\"\"\n",
    "    Generates an episode by executing the current policy in the given env\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    max_t = 1000 # max horizon within one episode\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # if isinstance(env.action_space, gym.spaces.Box):\n",
    "    #     max_possible_action = float(env.action_space.high[0]) # Only works with a action space dim of 1\n",
    "    #     min_possible_action = float(env.action_space.low[0]) # Only works with a action space dim of 1\n",
    "    #     max_possible_variance = max_possible_action - min_possible_action\n",
    "    #     max_acceptable_variance = 0.1 * max_possible_variance\n",
    "\n",
    "    for t in range(max_t):\n",
    "        print(t)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "        # if action space is discrete or continuous\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            probs = policy_net.forward(Variable(state)) # get each action choice probability with the current policy network\n",
    "            action = np.random.choice(env.action_space.n, p=np.squeeze(probs.detach().numpy())) # probablistic\n",
    "        else:\n",
    "            mean, log_stddev = policy_net.forward(Variable(state))\n",
    "            stddev = torch.clamp(torch.exp(log_stddev), min=0.01, max=1.0)\n",
    "            dist = torch.distributions.Normal(mean, stddev)\n",
    "            action = dist.sample()\n",
    "\n",
    "            # HERE BEFORE CHATGPT\n",
    "            # probs = policy_net.forward(Variable(state))\n",
    "            # #check if probs is NaN\n",
    "            # if torch.isnan(probs).any():\n",
    "            #     print(\"probs is NaN=\",probs)\n",
    "            #     probs = torch.zeros(probs.shape)\n",
    "            #     probs[0][0] = 0\n",
    "\n",
    "            # cov_mat = torch.eye(env.action_space.shape[0]) * 1  # Adjust covariance matrix dynamically or learn it\n",
    "\n",
    "            # dist = MultivariateNormal(probs, cov_mat)\n",
    "            # action = dist.sample().detach().numpy()[0][0]   # Only works with a action space dim of 1\n",
    "            # #check if action is within the action space\n",
    "            # if action < min_possible_action:\n",
    "            #     action = min_possible_action\n",
    "            # elif action > max_possible_action:\n",
    "            #     action = max_possible_action\n",
    "            # HERE BEFORE CHATGPT\n",
    "            \n",
    "\n",
    "        # action = np.argmax(probs.detach().numpy()) # greedy\n",
    "\n",
    "        #print(\"action=\",action)\n",
    "\n",
    "        # compute the log_prob to use this in parameter update\n",
    "        log_prob = None\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        else:\n",
    "            #log_prob = dist.log_prob(torch.tensor([action]))\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)  # Sum over action dimensions\n",
    "\n",
    "            \n",
    "        #print(log_prob)\n",
    "        \n",
    "        # append values\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # take a selected action\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, terminated, truncated, _ = env.step([action])\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated | truncated:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy_net):\n",
    "    \"\"\"\n",
    "    Compute accumulative trajectory reward\n",
    "    \"\"\"\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, clip_val=0.2, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Trains the policy network using PPO\n",
    "    \"\"\"\n",
    "    # Generate an episode with the current policy network\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net)\n",
    "    T = len(states)\n",
    "\n",
    "    # Create tensors depending on if it is discrete or continuous action space\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        actions = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    else:\n",
    "        actions = torch.FloatTensor(actions).to(device).view(-1,1)\n",
    "\n",
    "\n",
    "    states = np.vstack(states).astype(float)\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    #actions = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    rewards = torch.FloatTensor(rewards).to(device).view(-1,1)\n",
    "    log_probs = torch.FloatTensor(log_probs).to(device).view(-1,1)\n",
    "\n",
    "    # Compute total discounted return at each time step\n",
    "    Gs = []\n",
    "    G = 0\n",
    "    for t in range(T-1,-1,-1): # iterate in backward order to make the computation easier\n",
    "        G = rewards[t] + gamma*G\n",
    "        Gs.insert(0,G)\n",
    "    Gs = torch.tensor(Gs).view(-1,1)\n",
    "    \n",
    "    # Compute the advantage\n",
    "    state_vals = value_net(states).to(device)\n",
    "    with torch.no_grad():\n",
    "        A_k = Gs - state_vals\n",
    "        \n",
    "    for _ in range(num_epochs):\n",
    "        V = value_net(states).to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        # compute the log_prob to use it in parameter update\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            # Calculate probability of each action under the updated policy\n",
    "            probs = policy_net.forward(states).to(device)\n",
    "            curr_log_probs = torch.log(torch.gather(probs, 1, actions)) # Use torch.gather(A,1,B) to select columns from A based on indices in B\n",
    "        else:\n",
    "            # Calculate log probability of the sampled action\n",
    "            mean, log_stddev = policy_net.forward(states)\n",
    "            stddev = torch.clamp(torch.exp(log_stddev), min=0.01, max=1.0)\n",
    "            dist = torch.distributions.Normal(mean, stddev)\n",
    "            curr_log_probs = dist.log_prob(actions).sum(dim=-1)  # Sum over action dimensions\n",
    "            #curr_log_probs = torch.log(probs)\n",
    "        \n",
    "        # Calculate ratios r(theta)\n",
    "        ratios = torch.exp(curr_log_probs - log_probs)\n",
    "        \n",
    "        # Calculate two surrogate loss terms in cliped loss\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1-clip_val, 1+clip_val) * A_k\n",
    "        \n",
    "        # Calculate clipped loss value\n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean() # Need negative sign to run Gradient Ascent\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Update value net\n",
    "        critic_loss = nn.MSELoss()(V, Gs)\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "    return policy_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x1 and 3x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m value_net_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbaseline_lr)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_ite):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Train networks with PPO\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     policy_net, value_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_PPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m))\n",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m, in \u001b[0;36mtrain_PPO\u001b[1;34m(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, clip_val, gamma)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTrains the policy network using PPO\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate an episode with the current policy network\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m states, actions, rewards, log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_single_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(states)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create tensors depending on if it is discrete or continuous action space\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 58\u001b[0m, in \u001b[0;36mgenerate_single_episode\u001b[1;34m(env, policy_net)\u001b[0m\n\u001b[0;32m     56\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, p\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqueeze(probs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())) \u001b[38;5;66;03m# probablistic\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     mean, log_stddev \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     stddev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(torch\u001b[38;5;241m.\u001b[39mexp(log_stddev), \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     60\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mean, stddev)\n",
      "Cell \u001b[1;32mIn[39], line 27\u001b[0m, in \u001b[0;36mContinuousPolicyNet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x32)"
     ]
    }
   ],
   "source": [
    "# Define parameter values\n",
    "#env_name = 'CartPole-v0' # environment name\n",
    "#env_name = 'CartPole-v1' # environment name\n",
    "env_name = 'Pendulum-v1' # environment name\n",
    "\n",
    "#env_name = 'LunarLander-v2' # environment name\n",
    "#env_name = 'MountainCar-v0' # environment name\n",
    "#env_name = 'Acrobot-v1' # environment name\n",
    "\n",
    "\n",
    "num_train_ite = 600 # number of training iterations\n",
    "num_seeds = 1 # fit model with 3 different seeds and plot average performance of 3 seeds\n",
    "num_epochs = 10 # how many times we iterate the entire training dataset passing through the training\n",
    "eval_freq = 50 # run evaluation of policy at each eval_freq trials\n",
    "eval_epi_index = num_train_ite//eval_freq # use to create x label for plot\n",
    "returns = np.zeros((num_seeds, eval_epi_index))\n",
    "gamma = 0.99 # discount factor\n",
    "clip_val = 0.2 # hyperparameter epsilon in clip objective\n",
    "\n",
    "# Create the environment.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#detect if the environment is discrete or continuous\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nA = env.action_space.n\n",
    "else:\n",
    "    nA = env.action_space.shape[0]\n",
    "\n",
    "# detect if the environment state is discrete or continuous\n",
    "if isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.n\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "\n",
    "policy_lr = 5e-4 # policy network's learning rate \n",
    "baseline_lr = 1e-4 # value network's learning rate\n",
    " \n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # Define policy and value networks\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        policy_net = NeuralNet(nS, nA, torch.nn.Softmax()) # Normally this one works with CartPole-v0\n",
    "    else:\n",
    "        #policy_net = NeuralNet(nS, nA, torch.nn.Identity())\n",
    "        policy_net = ContinuousPolicyNet(nS, nA)\n",
    "        \n",
    "    policy_net_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "    value_net = NeuralNet(nS, 1, torch.nn.ReLU())\n",
    "    value_net_optimizer = optim.Adam(value_net.parameters(), lr=baseline_lr)\n",
    "    \n",
    "    for m in range(num_train_ite):\n",
    "        # Train networks with PPO\n",
    "        policy_net, value_net = train_PPO(env, policy_net, policy_net_optimizer, value_net, value_net_optimizer, num_epochs, clip_val=clip_val, gamma=gamma)\n",
    "        if m % eval_freq == 0:\n",
    "            print(\"Episode: {}\".format(m))\n",
    "            G = np.zeros(20)\n",
    "            for k in range(20):\n",
    "                g = evaluate_policy(env, policy_net)\n",
    "                G[k] = g\n",
    "\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(\"The avg. test reward for episode {0} is {1} with std of {2}.\".format(m, reward_mean, reward_sd))\n",
    "            reward_means.append(reward_mean)\n",
    "    returns[i] = np.array(reward_means)\n",
    "\n",
    "\n",
    "# Plot the performance over iterations\n",
    "x = np.arange(eval_epi_index)*eval_freq\n",
    "avg_returns = np.mean(returns, axis=0)\n",
    "max_returns = np.max(returns, axis=0)\n",
    "min_returns = np.min(returns, axis=0)\n",
    "\n",
    "plt.fill_between(x, min_returns, max_returns, alpha=0.1)\n",
    "plt.plot(x, avg_returns, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "plt.title(\"PPO Learning Curve\", fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1252830459db436a85b9800859f42f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb26d51d2d4e4bfcbb78ebc6d3d05f6b",
       "IPY_MODEL_248b8eb6d6d246e5b108afa59b6c53af"
      ],
      "layout": "IPY_MODEL_85ea1f2de43740659776d173f81b9e11"
     }
    },
    "171dadde77184657add3f9136b23b00e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "248b8eb6d6d246e5b108afa59b6c53af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3d9e9d1989d40438c07c1b652d747dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a0c39bd339746f794a67281a2442ad2",
      "value": 1
     }
    },
    "456be98214854aa2b6c3ac281a078ad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78c135b964044bcf841f6bb944e01b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9392daa2b25d4a27b681b7c836bdbf4a",
      "placeholder": "​",
      "style": "IPY_MODEL_456be98214854aa2b6c3ac281a078ad4",
      "value": "0.066 MB of 0.066 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "7a0c39bd339746f794a67281a2442ad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c4c2ae0c4454b07b3a5d63dca8d795c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85ea1f2de43740659776d173f81b9e11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8627cad6d7864563a89132995c034182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c4a0434c7a7413289ff097106d7318c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3a393a56c20445d848adc997aa8018e",
      "value": 1
     }
    },
    "9392daa2b25d4a27b681b7c836bdbf4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c4a0434c7a7413289ff097106d7318c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaecdcf1288446f9b7cd84f4f3044de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78c135b964044bcf841f6bb944e01b06",
       "IPY_MODEL_8627cad6d7864563a89132995c034182"
      ],
      "layout": "IPY_MODEL_171dadde77184657add3f9136b23b00e"
     }
    },
    "bb26d51d2d4e4bfcbb78ebc6d3d05f6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c4c2ae0c4454b07b3a5d63dca8d795c",
      "placeholder": "​",
      "style": "IPY_MODEL_c657703d2bce404d87e4dd41a28222d4",
      "value": "0.019 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "c657703d2bce404d87e4dd41a28222d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3a393a56c20445d848adc997aa8018e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3d9e9d1989d40438c07c1b652d747dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
