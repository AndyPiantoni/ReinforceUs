{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check and use GPU if available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch.distributions import MultivariateNormal, Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int, \n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(ContinuousActor, self).__init__()\n",
    "\n",
    "        self.hidden1 = nn.Linear(in_dim, 32)\n",
    "        self.hidden2 = nn.Linear(32, 32)\n",
    "        self.hidden3 = nn.Linear(32, 32)\n",
    "        self.mu_layer = nn.Linear(32, out_dim)\n",
    "        self.log_std_layer = nn.Linear(32, out_dim)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.tanh(self.log_std_layer(x))\n",
    "\n",
    "        std = torch.exp(log_std)\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actor = ContinuousActor(3, 1)\n",
    "action, dist = test_actor.forward(torch.tensor([1, 2, 3], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE from another notebook\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        self.output_activation = activation\n",
    "\n",
    "        # Initialization using Xavier normal (a popular technique for initializing weights in NNs)\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "def normalize_reward(env, reward):\n",
    "    \"\"\"\n",
    "    Normalize the reward between -1 and 1\n",
    "    \"\"\"\n",
    "    return (reward + 8.1) / 8.1\n",
    "\n",
    "def generate_single_episode(env, policy_net, eval=False):\n",
    "    \"\"\"\n",
    "    Generates an episode by executing the current policy in the given env\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    max_t = 1000 # max horizon within one episode\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        max_possible_action = float(env.action_space.high[0]) # Only works with a action space dim of 1\n",
    "        min_possible_action = float(env.action_space.low[0]) # Only works with a action space dim of 1\n",
    "\n",
    "    for t in range(max_t):\n",
    "        #print(t)\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        else:\n",
    "            state = torch.from_numpy(state.flatten()).float()\n",
    "            \n",
    "\n",
    "        # if action space is discrete or continuous\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            probs = policy_net.forward(Variable(state)) # get each action choice probability with the current policy network\n",
    "            action = np.random.choice(env.action_space.n, p=np.squeeze(probs.detach().numpy())) # probablistic\n",
    "        else:\n",
    "            action, dist = policy_net.forward(state) # continuous\n",
    "            # clip action to the action space\n",
    "            action = torch.clamp(action, min=min_possible_action, max=max_possible_action)\n",
    "            action = action.detach().numpy()\n",
    "            probs = dist\n",
    "\n",
    "\n",
    "        # compute the log_prob to use this in parameter update\n",
    "        log_prob = None\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        else:\n",
    "            log_prob = dist.log_prob(torch.tensor(action))\n",
    "            # print(f\"log_prob: {log_prob}\")\n",
    "            \n",
    "        #print(log_prob)\n",
    "        \n",
    "        # append values\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # take a selected action\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated:\n",
    "            print(f\"Envrionement solved in {t} steps\")\n",
    "        if not eval:\n",
    "            # normalize the reward between -1 and 1\n",
    "            # reward = (reward + 8.1) / 8.1\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                continue\n",
    "            else:\n",
    "                reward = normalize_reward(env, reward)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated | truncated:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy_net):\n",
    "    \"\"\"\n",
    "    Compute accumulative trajectory reward\n",
    "    \"\"\"\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net, eval=True)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, clip_val=0.2, gamma=0.99, entropy_coef=0.005, lamda=0.95):\n",
    "    \"\"\"\n",
    "    Trains the policy network using PPO\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate an episode with the current policy network\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net)\n",
    "    T = len(states)\n",
    "    \n",
    "    # Create tensors depending on if it is discrete or continuous action space\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        actions = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    else:\n",
    "        actions = torch.FloatTensor(actions).to(device).view(-1,1)\n",
    "\n",
    "\n",
    "    states = np.vstack(states).astype(float)\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device).view(-1,1)\n",
    "    log_probs = torch.FloatTensor(log_probs).to(device).view(-1,1)\n",
    "\n",
    "    # Compute the generalized advantage estimate\n",
    "    Gs = []\n",
    "    G = 0\n",
    "    for t in range(T-1,-1,-1):\n",
    "        delta = (rewards[t] + gamma*value_net(states[t]) - value_net(states[t-1]))\n",
    "        G = delta + gamma * G * lamda\n",
    "        Gs.insert(0,G)\n",
    "    Gs = torch.tensor(Gs).view(-1,1)\n",
    "    # for t in range(T-1,-1,-1): # iterate in backward order to make the computation easier\n",
    "    #     G = rewards[t] + gamma*G\n",
    "    #     Gs.insert(0,G)\n",
    "    # Gs = torch.tensor(Gs).view(-1,1)\n",
    "    \n",
    "    # Compute the advantage\n",
    "    state_vals = value_net(states).to(device)\n",
    "    with torch.no_grad():\n",
    "        A_k = Gs - state_vals\n",
    "        \n",
    "    for _ in range(num_epochs):\n",
    "        # Compute the value of the current states\n",
    "        V = value_net(states).to(device)\n",
    "\n",
    "\n",
    "        # Calculate probability of each action under the updated policy\n",
    "        # compute the log_prob to use it in parameter update\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            probs = policy_net.forward(states).to(device)\n",
    "            curr_log_probs = torch.log(torch.gather(probs, 1, actions))\n",
    "            # print(f\"probs, discrete: {probs}\")\n",
    "        else:\n",
    "            _, probs = policy_net.forward(states)\n",
    "            curr_log_probs = probs.log_prob(actions)\n",
    "            # print(f\"probs, continuous: {probs}\")\n",
    "\n",
    "        # Calculate ratios r(theta)\n",
    "        ratios = torch.exp(curr_log_probs - log_probs)\n",
    "        \n",
    "        # Calculate two surrogate loss terms in cliped loss\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1-clip_val, 1+clip_val) * A_k\n",
    "        \n",
    "        # entropy \n",
    "        if not isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            entropy = probs.entropy().mean()\n",
    "            actor_loss = (-torch.min(surr1, surr2).mean() - entropy_coef * entropy)\n",
    "        else:\n",
    "            # Calculate clipped loss value\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean() # Need negative sign to run Gradient Ascent\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Update value net\n",
    "        critic_loss = nn.MSELoss()(V, Gs)\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "    return policy_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter values\n",
    "# env_name = 'CartPole-v0' # environment name\n",
    "env_name = 'Pendulum-v1' # environment name\n",
    "# env_name = \"MountainCarContinuous-v0\"\n",
    "\n",
    "#env_name = 'LunarLander-v2' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "\n",
    "\n",
    "num_train_ite = 3000 # number of training iterations\n",
    "num_seeds = 1 # fit model with 3 different seeds and plot average performance of 3 seeds\n",
    "num_epochs = 10 # how many times we iterate the entire training dataset passing through the training\n",
    "eval_freq = 50 # run evaluation of policy at each eval_freq trials\n",
    "eval_epi_index = num_train_ite//eval_freq # use to create x label for plot\n",
    "returns = np.zeros((num_seeds, eval_epi_index))\n",
    "gamma = 0.99 # discount factor\n",
    "clip_val = 0.2 # hyperparameter epsilon in clip objective\n",
    "entropy_coef = 0.05 # hyperparameter entropy coefficient \n",
    "policy_lr = 1e-4 # policy network's learning rate \n",
    "baseline_lr = 5e-4 # value network's learning rate\n",
    "\n",
    "# Create the environment.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#detect if the environment is discrete or continuous\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nA = env.action_space.n\n",
    "else:\n",
    "    nA = env.action_space.shape[0]\n",
    "\n",
    "# detect if the environment state is discrete or continuous\n",
    "if isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.n\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    " \n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # Define policy and value networks\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        policy_net = NeuralNet(nS, nA, torch.nn.Softmax()) # Normally this one works with CartPole-v0\n",
    "    else:\n",
    "        #policy_net = NeuralNet(nS, nA, torch.nn.Identity())\n",
    "        policy_net = ContinuousActor(nS, nA)\n",
    "    policy_net_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "    value_net = NeuralNet(nS, 1, torch.nn.ReLU())\n",
    "    value_net_optimizer = optim.Adam(value_net.parameters(), lr=baseline_lr)\n",
    "    \n",
    "    for m in range(num_train_ite):\n",
    "        # Train networks with PPO\n",
    "        policy_net, value_net = train_PPO(env, policy_net, policy_net_optimizer, value_net, value_net_optimizer, num_epochs, clip_val=clip_val, gamma=gamma, entropy_coef=entropy_coef)\n",
    "        if m % eval_freq == 0:\n",
    "            print(\"Episode: {}\".format(m))\n",
    "            G = np.zeros(20)\n",
    "            for k in range(20):\n",
    "                g = evaluate_policy(env, policy_net)\n",
    "                G[k] = g\n",
    "\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(\"The avg. test reward for episode {0} is {1} with std of {2}.\".format(m, reward_mean, reward_sd))\n",
    "            reward_means.append(reward_mean)\n",
    "    returns[i] = np.array(reward_means)\n",
    "\n",
    "# save the policy network\n",
    "torch.save(policy_net.state_dict(), f\"PPO_policy_net_{env_name}.pt\")\n",
    "\n",
    "\n",
    "# Plot the performance over iterations\n",
    "x = np.arange(eval_epi_index)*eval_freq\n",
    "avg_returns = np.mean(returns, axis=0)\n",
    "max_returns = np.max(returns, axis=0)\n",
    "min_returns = np.min(returns, axis=0)\n",
    "\n",
    "plt.fill_between(x, min_returns, max_returns, alpha=0.1)\n",
    "plt.plot(x, avg_returns, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "plt.title(\"PPO Learning Curve\", fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#create an environment to test and visualize the policy\n",
    "# env_name = 'CartPole-v0' # environment name\n",
    "env_name = 'Pendulum-v1' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "# env_name = \"MountainCarContinuous-v0\"\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.n\n",
    "    policy_net = NeuralNet(nS, nA, torch.nn.Softmax())\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.shape[0]\n",
    "    policy_net = ContinuousActor(nS, nA)\n",
    "\n",
    "# load the policy network\n",
    "policy_net.load_state_dict(torch.load(f\"PPO_policy_net_{env_name}.pt\"))\n",
    "# policy_net.eval()\n",
    "\n",
    "episodes = 5\n",
    "state = env.reset()[0]\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "for i in range(episodes):\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action, _ = policy_net.forward(torch.tensor(state, dtype=torch.float32))\n",
    "        # clip action to the action space\n",
    "        action = torch.clamp(action, min=-2, max=2)\n",
    "        \n",
    "        # take a selected action\n",
    "        action = action.detach().numpy()\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "        state = torch.from_numpy(state.flatten()).float()\n",
    "        rewards.append(reward)\n",
    "    avg_rewards.append(np.sum(rewards))\n",
    "    print(f\"Episode {i}: {np.sum(rewards)}\")\n",
    "    env.reset()\n",
    "print(f\"Average reward: {np.mean(avg_rewards)} over {episodes} episodes\")\n",
    "env.close()\n",
    "    \n",
    "    \n",
    "# for i in range(episodes):\n",
    "#     try:\n",
    "#         reward = evaluate_policy(env, policy_net)\n",
    "#     except Exception as e:\n",
    "#         # print error and traceback\n",
    "#         print(e)\n",
    "        \n",
    "#         env.close()\n",
    "\n",
    "#     print(f\"Episode {i}: {reward}\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Pendulum-v1' # environment name\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "action = 2\n",
    "state = env.reset()[0]\n",
    "for i in range(200):\n",
    "    state, reward, done, truncated, _ = env.step([action])\n",
    "    if done or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1252830459db436a85b9800859f42f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb26d51d2d4e4bfcbb78ebc6d3d05f6b",
       "IPY_MODEL_248b8eb6d6d246e5b108afa59b6c53af"
      ],
      "layout": "IPY_MODEL_85ea1f2de43740659776d173f81b9e11"
     }
    },
    "171dadde77184657add3f9136b23b00e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "248b8eb6d6d246e5b108afa59b6c53af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3d9e9d1989d40438c07c1b652d747dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a0c39bd339746f794a67281a2442ad2",
      "value": 1
     }
    },
    "456be98214854aa2b6c3ac281a078ad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78c135b964044bcf841f6bb944e01b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9392daa2b25d4a27b681b7c836bdbf4a",
      "placeholder": "​",
      "style": "IPY_MODEL_456be98214854aa2b6c3ac281a078ad4",
      "value": "0.066 MB of 0.066 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "7a0c39bd339746f794a67281a2442ad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c4c2ae0c4454b07b3a5d63dca8d795c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85ea1f2de43740659776d173f81b9e11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8627cad6d7864563a89132995c034182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c4a0434c7a7413289ff097106d7318c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3a393a56c20445d848adc997aa8018e",
      "value": 1
     }
    },
    "9392daa2b25d4a27b681b7c836bdbf4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c4a0434c7a7413289ff097106d7318c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaecdcf1288446f9b7cd84f4f3044de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78c135b964044bcf841f6bb944e01b06",
       "IPY_MODEL_8627cad6d7864563a89132995c034182"
      ],
      "layout": "IPY_MODEL_171dadde77184657add3f9136b23b00e"
     }
    },
    "bb26d51d2d4e4bfcbb78ebc6d3d05f6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c4c2ae0c4454b07b3a5d63dca8d795c",
      "placeholder": "​",
      "style": "IPY_MODEL_c657703d2bce404d87e4dd41a28222d4",
      "value": "0.019 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "c657703d2bce404d87e4dd41a28222d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3a393a56c20445d848adc997aa8018e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3d9e9d1989d40438c07c1b652d747dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
