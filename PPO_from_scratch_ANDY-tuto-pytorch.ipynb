{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check and use GPU if available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torch.distributions import MultivariateNormal, Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_dim: int, \n",
    "        out_dim: int, \n",
    "    ):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(ContinuousActor, self).__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(in_dim, 32)\n",
    "        self.mu_layer = nn.Linear(32, out_dim)\n",
    "        self.log_std_layer = nn.Linear(32, out_dim)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden(state))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = torch.tanh(self.log_std_layer(x))\n",
    "\n",
    "        std = torch.exp(log_std)\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actor = ContinuousActor(3, 1)\n",
    "action, dist = test_actor.forward(torch.tensor([1, 2, 3], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE from another notebook\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        self.output_activation = activation\n",
    "\n",
    "        # Initialization using Xavier normal (a popular technique for initializing weights in NNs)\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_single_episode(env, policy_net):\n",
    "    \"\"\"\n",
    "    Generates an episode by executing the current policy in the given env\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    max_t = 1000 # max horizon within one episode\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        max_possible_action = float(env.action_space.high[0]) # Only works with a action space dim of 1\n",
    "        min_possible_action = float(env.action_space.low[0]) # Only works with a action space dim of 1\n",
    "\n",
    "    for t in range(max_t):\n",
    "        #print(t)\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        else:\n",
    "            state = torch.from_numpy(state.flatten()).float()\n",
    "            \n",
    "\n",
    "        # if action space is discrete or continuous\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            probs = policy_net.forward(Variable(state)) # get each action choice probability with the current policy network\n",
    "            action = np.random.choice(env.action_space.n, p=np.squeeze(probs.detach().numpy())) # probablistic\n",
    "        else:\n",
    "            action, dist = policy_net.forward(state) # continuous\n",
    "            # clip action to the action space\n",
    "            action = torch.clamp(action, min=min_possible_action, max=max_possible_action)\n",
    "            action = action.detach().numpy()\n",
    "            probs = dist\n",
    "\n",
    "\n",
    "        # compute the log_prob to use this in parameter update\n",
    "        log_prob = None\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        else:\n",
    "            log_prob = dist.log_prob(torch.tensor(action))\n",
    "            # print(f\"log_prob: {log_prob}\")\n",
    "            \n",
    "        #print(log_prob)\n",
    "        \n",
    "        # append values\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # take a selected action\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            #reward = (reward + 8.1) / 8.1\n",
    "        else:\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            reward = (reward + 8.1) / 8.1\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if terminated | truncated:\n",
    "            break\n",
    "            \n",
    "    return states, actions, rewards, log_probs\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy_net):\n",
    "    \"\"\"\n",
    "    Compute accumulative trajectory reward\n",
    "    \"\"\"\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, clip_val=0.2, gamma=0.99, entropy_coef=0.005, lamda=0.95):\n",
    "    \"\"\"\n",
    "    Trains the policy network using PPO\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate an episode with the current policy network\n",
    "    states, actions, rewards, log_probs = generate_single_episode(env, policy_net)\n",
    "    T = len(states)\n",
    "    \n",
    "    # Create tensors depending on if it is discrete or continuous action space\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        actions = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    else:\n",
    "        actions = torch.FloatTensor(actions).to(device).view(-1,1)\n",
    "\n",
    "\n",
    "    states = np.vstack(states).astype(float)\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device).view(-1,1)\n",
    "    log_probs = torch.FloatTensor(log_probs).to(device).view(-1,1)\n",
    "\n",
    "    # Compute the generalized advantage estimate\n",
    "    Gs = []\n",
    "    G = 0\n",
    "    for t in range(T-1,-1,-1):\n",
    "        delta = (rewards[t] + gamma*value_net(states[t]) - value_net(states[t-1]))\n",
    "        G = delta + gamma * G * lamda\n",
    "        Gs.insert(0,G)\n",
    "    Gs = torch.tensor(Gs).view(-1,1)\n",
    "    # for t in range(T-1,-1,-1): # iterate in backward order to make the computation easier\n",
    "    #     G = rewards[t] + gamma*G\n",
    "    #     Gs.insert(0,G)\n",
    "    # Gs = torch.tensor(Gs).view(-1,1)\n",
    "    \n",
    "    # Compute the advantage\n",
    "    state_vals = value_net(states).to(device)\n",
    "    with torch.no_grad():\n",
    "        A_k = Gs - state_vals\n",
    "        \n",
    "    for _ in range(num_epochs):\n",
    "        # Compute the value of the current states\n",
    "        V = value_net(states).to(device)\n",
    "\n",
    "\n",
    "        # Calculate probability of each action under the updated policy\n",
    "        # compute the log_prob to use it in parameter update\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            probs = policy_net.forward(states).to(device)\n",
    "            curr_log_probs = torch.log(torch.gather(probs, 1, actions))\n",
    "            # print(f\"probs, discrete: {probs}\")\n",
    "        else:\n",
    "            _, probs = policy_net.forward(states)\n",
    "            curr_log_probs = probs.log_prob(actions)\n",
    "            # print(f\"probs, continuous: {probs}\")\n",
    "\n",
    "        # Calculate ratios r(theta)\n",
    "        ratios = torch.exp(curr_log_probs - log_probs)\n",
    "        \n",
    "        # Calculate two surrogate loss terms in cliped loss\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1-clip_val, 1+clip_val) * A_k\n",
    "        \n",
    "        # entropy \n",
    "        if not isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            entropy = probs.entropy().mean()\n",
    "            actor_loss = (-torch.min(surr1, surr2).mean() - entropy_coef * entropy)\n",
    "        else:\n",
    "            # Calculate clipped loss value\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean() # Need negative sign to run Gradient Ascent\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Update value net\n",
    "        critic_loss = nn.MSELoss()(V, Gs)\n",
    "        value_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "    return policy_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]c:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The avg. test reward for episode 0 is 14.95 with std of 4.165033013074447.\n",
      "Episode: 50\n",
      "The avg. test reward for episode 50 is 21.5 with std of 11.399561395071302.\n",
      "Episode: 100\n",
      "The avg. test reward for episode 100 is 38.05 with std of 19.779977249734134.\n",
      "Episode: 150\n",
      "The avg. test reward for episode 150 is 50.4 with std of 27.59057810195357.\n",
      "Episode: 200\n",
      "The avg. test reward for episode 200 is 67.45 with std of 46.94941426684682.\n",
      "Episode: 250\n",
      "The avg. test reward for episode 250 is 133.95 with std of 49.69152342200831.\n",
      "Episode: 300\n",
      "The avg. test reward for episode 300 is 160.6 with std of 37.14888962001422.\n",
      "Episode: 350\n",
      "The avg. test reward for episode 350 is 171.55 with std of 32.90056990387857.\n",
      "Episode: 400\n",
      "The avg. test reward for episode 400 is 169.9 with std of 36.19240251765555.\n",
      "Episode: 450\n",
      "The avg. test reward for episode 450 is 192.75 with std of 21.16335275895575.\n",
      "Episode: 500\n",
      "The avg. test reward for episode 500 is 196.3 with std of 12.272326592785902.\n",
      "Episode: 550\n",
      "The avg. test reward for episode 550 is 191.15 with std of 20.318156904601363.\n",
      "Episode: 600\n",
      "The avg. test reward for episode 600 is 168.5 with std of 21.29906101216671.\n",
      "Episode: 650\n",
      "The avg. test reward for episode 650 is 185.6 with std of 37.6342928723259.\n",
      "Episode: 700\n",
      "The avg. test reward for episode 700 is 195.75 with std of 18.525320510047862.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [02:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m value_net_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbaseline_lr)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_ite):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Train networks with PPO\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     policy_net, value_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_PPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentropy_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m))\n",
      "Cell \u001b[1;32mIn[31], line 48\u001b[0m, in \u001b[0;36mtrain_PPO\u001b[1;34m(env, policy_net, policy_optimizer, value_net, value_optimizer, num_epochs, clip_val, gamma, entropy_coef, lamda)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Calculate probability of each action under the updated policy\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# compute the log_prob to use it in parameter update\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m---> 48\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     49\u001b[0m     curr_log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mgather(probs, \u001b[38;5;241m1\u001b[39m, actions))\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# print(f\"probs, discrete: {probs}\")\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[30], line 27\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(inputs))\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x))\n\u001b[1;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x))\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\Rlearn\\Lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define parameter values\n",
    "env_name = 'CartPole-v0' # environment name\n",
    "#env_name = 'Pendulum-v1' # environment name\n",
    "\n",
    "#env_name = 'LunarLander-v2' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "\n",
    "\n",
    "num_train_ite = 1000 # number of training iterations\n",
    "num_seeds = 1 # fit model with 3 different seeds and plot average performance of 3 seeds\n",
    "num_epochs = 10 # how many times we iterate the entire training dataset passing through the training\n",
    "eval_freq = 50 # run evaluation of policy at each eval_freq trials\n",
    "eval_epi_index = num_train_ite//eval_freq # use to create x label for plot\n",
    "returns = np.zeros((num_seeds, eval_epi_index))\n",
    "gamma = 0.9 # discount factor\n",
    "clip_val = 0.2 # hyperparameter epsilon in clip objective\n",
    "entropy_coef = 0.00 # hyperparameter entropy coefficient (o.005)\n",
    "\n",
    "# Create the environment.\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#detect if the environment is discrete or continuous\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nA = env.action_space.n\n",
    "else:\n",
    "    nA = env.action_space.shape[0]\n",
    "\n",
    "# detect if the environment state is discrete or continuous\n",
    "if isinstance(env.observation_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.n\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "\n",
    "policy_lr = 1e-4 # policy network's learning rate \n",
    "baseline_lr = 5e-4 # value network's learning rate\n",
    " \n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # Define policy and value networks\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        policy_net = NeuralNet(nS, nA, torch.nn.Softmax()) # Normally this one works with CartPole-v0\n",
    "    else:\n",
    "        #policy_net = NeuralNet(nS, nA, torch.nn.Identity())\n",
    "        policy_net = ContinuousActor(nS, nA)\n",
    "    policy_net_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "    value_net = NeuralNet(nS, 1, torch.nn.ReLU())\n",
    "    value_net_optimizer = optim.Adam(value_net.parameters(), lr=baseline_lr)\n",
    "    \n",
    "    for m in range(num_train_ite):\n",
    "        # Train networks with PPO\n",
    "        policy_net, value_net = train_PPO(env, policy_net, policy_net_optimizer, value_net, value_net_optimizer, num_epochs, clip_val=clip_val, gamma=gamma, entropy_coef=entropy_coef)\n",
    "        if m % eval_freq == 0:\n",
    "            print(\"Episode: {}\".format(m))\n",
    "            G = np.zeros(20)\n",
    "            for k in range(20):\n",
    "                g = evaluate_policy(env, policy_net)\n",
    "                G[k] = g\n",
    "\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(\"The avg. test reward for episode {0} is {1} with std of {2}.\".format(m, reward_mean, reward_sd))\n",
    "            reward_means.append(reward_mean)\n",
    "    returns[i] = np.array(reward_means)\n",
    "\n",
    "# save the policy network\n",
    "torch.save(policy_net.state_dict(), f\"policy_net_{env_name}.pt\")\n",
    "\n",
    "\n",
    "# Plot the performance over iterations\n",
    "x = np.arange(eval_epi_index)*eval_freq\n",
    "avg_returns = np.mean(returns, axis=0)\n",
    "max_returns = np.max(returns, axis=0)\n",
    "min_returns = np.min(returns, axis=0)\n",
    "\n",
    "plt.fill_between(x, min_returns, max_returns, alpha=0.1)\n",
    "plt.plot(x, avg_returns, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "plt.title(\"PPO Learning Curve\", fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the policy network\n",
    "torch.save(policy_net.state_dict(), f\"policy_net_{env_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_23236\\848387439.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action, _ = policy_net.forward(torch.tensor(state, dtype=torch.float32))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: -856.0091870798767\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#create an environment to test and visualize the policy\n",
    "# env_name = 'CartPole-v0' # environment name\n",
    "env_name = 'Pendulum-v1' # environment name\n",
    "# env_name = 'MountainCar-v0' # environment name\n",
    "# env_name = 'Acrobot-v1' # environment name\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.n\n",
    "    policy_net = NeuralNet(nS, nA, torch.nn.Softmax())\n",
    "else:\n",
    "    nS = env.observation_space.shape[0]\n",
    "    nA = env.action_space.shape[0]\n",
    "    policy_net = ContinuousActor(nS, nA)\n",
    "\n",
    "# load the policy network\n",
    "policy_net.load_state_dict(torch.load(f\"policy_net_{env_name}.pt\"))\n",
    "# policy_net.eval()\n",
    "\n",
    "episodes = 1\n",
    "state = env.reset()[0]\n",
    "rewards = []\n",
    "for i in range(episodes):\n",
    "    while True:\n",
    "        action, _ = policy_net.forward(torch.tensor(state, dtype=torch.float32))\n",
    "        # clip action to the action space\n",
    "        action = torch.clamp(action, min=-2, max=2)\n",
    "        \n",
    "        # take a selected action\n",
    "        action = action.detach().numpy()\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "        state = torch.from_numpy(state.flatten()).float()\n",
    "        rewards.append(reward)\n",
    "    print(f\"Episode {i}: {np.sum(rewards)}\")\n",
    "    env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1252830459db436a85b9800859f42f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb26d51d2d4e4bfcbb78ebc6d3d05f6b",
       "IPY_MODEL_248b8eb6d6d246e5b108afa59b6c53af"
      ],
      "layout": "IPY_MODEL_85ea1f2de43740659776d173f81b9e11"
     }
    },
    "171dadde77184657add3f9136b23b00e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "248b8eb6d6d246e5b108afa59b6c53af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3d9e9d1989d40438c07c1b652d747dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a0c39bd339746f794a67281a2442ad2",
      "value": 1
     }
    },
    "456be98214854aa2b6c3ac281a078ad4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78c135b964044bcf841f6bb944e01b06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9392daa2b25d4a27b681b7c836bdbf4a",
      "placeholder": "​",
      "style": "IPY_MODEL_456be98214854aa2b6c3ac281a078ad4",
      "value": "0.066 MB of 0.066 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "7a0c39bd339746f794a67281a2442ad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c4c2ae0c4454b07b3a5d63dca8d795c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85ea1f2de43740659776d173f81b9e11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8627cad6d7864563a89132995c034182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c4a0434c7a7413289ff097106d7318c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3a393a56c20445d848adc997aa8018e",
      "value": 1
     }
    },
    "9392daa2b25d4a27b681b7c836bdbf4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c4a0434c7a7413289ff097106d7318c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaecdcf1288446f9b7cd84f4f3044de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78c135b964044bcf841f6bb944e01b06",
       "IPY_MODEL_8627cad6d7864563a89132995c034182"
      ],
      "layout": "IPY_MODEL_171dadde77184657add3f9136b23b00e"
     }
    },
    "bb26d51d2d4e4bfcbb78ebc6d3d05f6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c4c2ae0c4454b07b3a5d63dca8d795c",
      "placeholder": "​",
      "style": "IPY_MODEL_c657703d2bce404d87e4dd41a28222d4",
      "value": "0.019 MB of 0.019 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "c657703d2bce404d87e4dd41a28222d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3a393a56c20445d848adc997aa8018e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3d9e9d1989d40438c07c1b652d747dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
