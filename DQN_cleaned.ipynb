{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, an implementation for the DQN algorithm takes place (hopefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install notes: \n",
    "\n",
    "pip install pytorch\n",
    "\n",
    "pip install matplotlib\n",
    "\n",
    "pip install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Algorithm we want to implement is Deep-Q learning with Experience Replay\n",
    "The pseudocode outline of the Algorithm can be seen below:\n",
    "\n",
    "\n",
    "&ensp;Initialize replay memory D to capacity N<br>\n",
    "&ensp;Initialize action-value function Q with random weights<br>\n",
    "&ensp;**for** episode = 1, M **do** <br>\n",
    "&ensp;&ensp;&ensp; Initialise sequence $s1 = {x1}$ and preprocessed sequenced $\\phi_1 = \\phi(s1)$<br>\n",
    "&ensp;&ensp;&ensp; **for** t = 1, T **do** <br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; With probability $\\epsilon$ select a random action $a_t$<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; otherwise select $a_t = max_a Q^*(\\phi(s_t),a;\\theta)$<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; Set $s_{t+1} = s_t ,a_t ,x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; Store transition $(\\phi_t ,a_t,r_t,\\phi_{t+1})$ in D<br>\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; Sample random minibatch of transitions $(\\phi_j , a_j, r_j, \\phi_{j+1})$ from D<br>\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;$\n",
    "\\text{Set} \\ y = \\begin{cases}\n",
    "r_j, & \\text{for terminal} \\ \\phi_{j+1} \\\\\n",
    "r_j+\\gamma \\ max_{a^{'}}Q(\\phi_{j+1}, a';\\theta), & \\text{for non-terminal } \\ \\phi_{j+1}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;Perform a gradient descent step on $(y_j − Q(\\phi_j, a_j; \\theta))²$<br>\n",
    "&ensp;&ensp;&ensp;**end for**<br>\n",
    "&ensp;**end for**\n",
    "\n",
    "The gradient descent step is given by:<br>\n",
    "$\\nabla_{\\theta_i}L_i(\\theta_i) = \\mathbb{E}_{s, a \\sim p(\\cdot);s'\\sim \\epsilon} \\big[\\big(r+\\gamma \\max_{a'} Q(s', a';\\theta_{i-1})-Q(s,a;\\theta_i)\\big)\\nabla_{\\theta_i}(s,a;\\theta_i) \\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notes: <br>\n",
    "3 processes:<br>\n",
    "- Process 1: Data Aquisition to fill buffer (latest Q function with some exploration)\n",
    "- Process 2: Updates Target parameters (slower than Process 1 and 3, like every 10k iterations), copies $\\phi$ into $\\phi'$, with $\\phi'$ being the current target\n",
    "- Process 3: Actuall Training, fetch data from buffer, update our Q function $\\phi$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and global variables\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "# BATCH_SIZE = 256\n",
    "# CAPACITY = 10000 # Capacity of Replay Memory Buffer\n",
    "CAPACITY = 40000 # Capacity of Replay Memory Buffer\n",
    "GAMMA = 0.99\n",
    "eps_start = 0.9 # Epsilon Greedy with decay rate based on steps taken for exploration\n",
    "eps_end = 0.05\n",
    "# EPS_DECAY = 1000\n",
    "eps_decay = 1000 # Epsilon Greedy decay rate, varaible to be modified for each case\n",
    "# TAU = 0.005 # Instead of doing a hard update from the policy network to the target network, we do a soft update every iteration\n",
    "TAU = 0.001\n",
    "LR = 1e-4\n",
    "\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_old(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_observations, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(32,32), \n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, policy_net, state, actions, cts = False, explore=True):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = eps_end + (eps_start - eps_end) * \\\n",
    "        math.exp(-1. * steps_done / eps_decay)\n",
    "    if not explore:\n",
    "        eps_threshold = 0\n",
    "    else:\n",
    "        steps_done += 1\n",
    "    if sample > eps_threshold: # Greedy\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else: # Exploration\n",
    "        if cts:\n",
    "            return torch.tensor([np.digitize(env.action_space.sample(), actions)], device=device, dtype=torch.long)\n",
    "        else:\n",
    "            return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 2:\n",
    "        means = durations_t.unfold(0, 2, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(1), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(rewards_list, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Rewards')\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(rewards_t) >= 2:\n",
    "        plt.plot(rewards_t.numpy())\n",
    "        means = rewards_t.unfold(0, 2, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(1), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning Performance\n",
    "\n",
    "def plot_learning(eval_epi_index, eval_freq, rewards, avg_returns, max_returns, min_returns, show_result=False):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    x = eval_epi_index\n",
    "\n",
    "    plt.fill_between(x, min_returns, max_returns, alpha=0.1)\n",
    "    plt.plot(x, avg_returns, '-o', markersize=1)\n",
    "\n",
    "    plt.xlabel('Episode', fontsize=15)\n",
    "    plt.ylabel('Return', fontsize=15)\n",
    "\n",
    "    plt.title(\"DQN Learning Curve\", fontsize=24)\n",
    "    plt.pause(0.001)\n",
    "    if not show_result:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta_i}L_i(\\theta_i) = \\mathbb{E}_{s, a \\sim p(\\cdot);s'\\sim \\epsilon} \\big[\\big(r+\\gamma \\max_{a'} Q(s', a';\\theta_{i-1})-Q(s,a;\\theta_i)\\big)\\nabla_{\\theta_i}(s,a;\\theta_i) \\big]$\n",
    "\n",
    "we want to minimize: (loss_function): (the gradient of this equivalents to backpropagating the loss on the network)\n",
    "\n",
    "$(y_j − Q(\\phi_j, a_j; \\theta))²$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\text{Set} \\ y = \\begin{cases}\n",
    "r_j, & \\text{for terminal} \\ \\phi_{j+1} \\\\\n",
    "r_j+\\gamma \\ max_{a^{'}}Q(\\phi_{j+1}, a';\\theta), & \\text{for non-terminal } \\ \\phi_{j+1}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Q_values, target_Q_values, rewards, done, actions):\n",
    "    # Calculate the target values\n",
    "    if done:\n",
    "        y = rewards\n",
    "    else:\n",
    "        y = rewards + GAMMA * torch.max(target_Q_values, dim=-1)[0] \n",
    "\n",
    "    # Calculate the temporal difference error\n",
    "    TD_error = y - Q_values\n",
    "    # Square the TD error\n",
    "    squared_TD_error = TD_error ** 2\n",
    "\n",
    "    # Calculate the mean squared TD error\n",
    "    loss = torch.mean(squared_TD_error)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(policy_net, target_net, optimizer, memory):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # loss = F.huber_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # loss = loss_function(state_action_values, expected_state_action_values.unsqueeze(1), reward_batch, 0, action_batch)\n",
    "\n",
    "    # Optimize the model, i.e. gradient update\n",
    "    optimizer.zero_grad() # Practicality dictates that we zero the gradients before backpropagation\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1)\n",
    "    # torch.nn.utils.clip_grad_value_(policy_net.parameters(), 50) # worked decent on CartPole\n",
    "    # torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, num_episodes, policy_net, target_net, memory, optimizer, actions, device, eval_freq, cts=False):\n",
    "    avg_returns = []\n",
    "    max_returns = []\n",
    "    min_returns = []\n",
    "    eval_epi_index = []\n",
    "    \n",
    "    for i_episode in range(num_episodes+1):\n",
    "        # Initialize the environment and get its state\n",
    "        total_reward = 0\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        for t in count():\n",
    "            action = select_action(env, policy_net, state, actions, cts) \n",
    "            # action = select_action(env, target_net, state, actions, cts) # We sample environment with older network, aka target net\n",
    "            # print(action.item())\n",
    "            if cts:\n",
    "                observation, reward, terminated, truncated, _ = env.step([actions[action.item()]])\n",
    "            else:\n",
    "                observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "            # env.render()\n",
    "            total_reward+=reward\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "                \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            optimize_model(policy_net, target_net, optimizer, memory)\n",
    "\n",
    "\n",
    "            # Soft update of the target network's weights, instead of hard update every X iterations\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            # target_net_state_dict = target_net.state_dict()\n",
    "            # policy_net_state_dict = policy_net.state_dict()\n",
    "            # for key in policy_net_state_dict:\n",
    "            #     target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            # target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                # print(f\"Total reward: {total_reward}\")\n",
    "                episode_durations.append(t + 1)\n",
    "                rewards_list.append(total_reward)\n",
    "                # plot_durations()\n",
    "                # plot_rewards()\n",
    "                break\n",
    "        \n",
    "        # if i_episode % int(1/TAU/2) == 0:\n",
    "        if i_episode % eval_freq == 0:\n",
    "            # print(f\"Episode : {i_episode} hardupdate\")\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            all_reward = []\n",
    "            for i in range(20):\n",
    "                # run trained model and print average reward over the 5 episodes\n",
    "                state, info = env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                total_reward = 0\n",
    "                for t in count():\n",
    "                    action = select_action(env, target_net, state, env.action_space, cts=False, explore=False)\n",
    "                    # action = select_action(env, policy_net, state, env.action_space, cts=False, explore=False)\n",
    "                    if cts:\n",
    "                        observation, reward, terminated, truncated, _ = env.step([actions[action.item()]])\n",
    "                    else:\n",
    "                        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                    total_reward += reward\n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                    state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                all_reward.append(total_reward)\n",
    "            avg_returns.append(np.mean(all_reward, axis=0))\n",
    "            max_returns.append(np.max(all_reward, axis=0))\n",
    "            min_returns.append(np.min(all_reward, axis=0))\n",
    "            eval_epi_index.append(i_episode+1)\n",
    "            plot_learning(eval_epi_index, eval_freq, all_reward, avg_returns, max_returns, min_returns)\n",
    "        \n",
    "        # if (i_episode) % eval_freq == 0:# and i_episode > 1:\n",
    "            \n",
    "            # rewards = rewards_list[((i_episode+1)-eval_freq+1):(i_episode+1)]\n",
    "            # avg_returns.append(np.mean(rewards, axis=0))\n",
    "            # max_returns.append(np.max(rewards, axis=0))\n",
    "            # min_returns.append(np.min(rewards, axis=0))\n",
    "            # eval_epi_index.append(i_episode+1)\n",
    "            # plot_learning(eval_epi_index, eval_freq, all_reward, avg_returns, max_returns, min_returns)\n",
    "\n",
    "    print('Complete')\n",
    "    plot_rewards(show_result=True)\n",
    "    plot_learning(eval_epi_index, eval_freq, rewards_list, avg_returns, max_returns, min_returns, show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First environment: Mountain Car\n",
    "---\n",
    "\n",
    "Action Space: Discrete (3)\n",
    "\n",
    "Observation Space: Box([-1.2 -0.07], [0.6 0.07], (2,), float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "rewards_list = []\n",
    "\n",
    "eps_decay = 50000 # Mountain-car needs alot of exploration as only the target state gives a different reward\n",
    "eps_start = 0.9 # Epsilon Greedy with decay rate based on steps taken for exploration\n",
    "eps_end = 0.05\n",
    "\n",
    "num_episodes = 1000 # number of training iterations\n",
    "eval_freq = 20 # sumarize trained episodes frequency\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "# call the train_function, move everything below into callable functions\n",
    "\n",
    "train_model(env, num_episodes, policy_net, target_net, memory, optimizer, env.action_space, device, eval_freq, cts=False)\n",
    "torch.save(policy_net.state_dict(), f\"DQN_policy_net_Mountaincar.pt\") # look at what to save in the end\n",
    "torch.save(target_net.state_dict(), f\"DQN_target_net_Mountaincar.pt\") # look at what to save in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target net\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(torch.load(\"DQN_policy_net_Mountaincar.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Plotting for non-Linux-nerds\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "# run trained model on environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in count():\n",
    "    action = select_action(env, target_net, state, env.action_space, cts=False, explore=False)\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "all_reward = []\n",
    "for i in range(5):\n",
    "    # run trained model and print average reward over the 5 episodes\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(env, target_net, state, env.action_space, cts=False, explore=False)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    all_reward.append(total_reward)\n",
    "env.close()\n",
    "print(all_reward)\n",
    "print(sum(all_reward)/len(all_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second environment: Cartpole\n",
    "---\n",
    "\n",
    "Action Space: Discrete (2)\n",
    "\n",
    "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "rewards_list = []\n",
    "\n",
    "eps_decay = 100000\n",
    "eps_start = 0.9 # Epsilon Greedy with decay rate based on steps taken for exploration\n",
    "# eps_end = 0.05\n",
    "eps_end = 0.001\n",
    "\n",
    "num_episodes = 10000 # number of training iterations\n",
    "eval_freq = 20 # sumarize trained episodes frequency\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "# call the train_function, move everything below into callable functions\n",
    "\n",
    "train_model(env, num_episodes, policy_net, target_net, memory, optimizer, env.action_space, device, eval_freq, cts=False)\n",
    "torch.save(policy_net.state_dict(), f\"DQN_policy_net_Cartpole.pt\") # look at what to save in the end\n",
    "torch.save(target_net.state_dict(), f\"DQN_target_net_Cartpole.pt\") # look at what to save in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target net\n",
    "env = gym.make(\"CartPole-V1\")\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(torch.load(\"DQN_policy_net_Cartpole.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Plotting for non-Linux-nerds\n",
    "\n",
    "env = gym.make(\"CartPole-V1\", render_mode='human')\n",
    "\n",
    "# run trained model on environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in count():\n",
    "    action = select_action(env, target_net, state, env.action_space, cts=False, explore=False)\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "all_reward = []\n",
    "for i in range(2000):\n",
    "    # run trained model and print average reward over the 5 episodes\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(env, target_net, state, env.action_space, cts=False, explore=False)\n",
    "        # action = select_action(env, policy_net, state, env.action_space, cts=False, explore=False)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    all_reward.append(total_reward)\n",
    "env.close()\n",
    "print(f\"Max Reward: {max(all_reward)}, Min Reward: {min(all_reward)}, Average Reward: {sum(all_reward)/len(all_reward)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third enviroment: pendulum\n",
    "--\n",
    "\n",
    "Action Space: Box(-2.0, 2.0, (1,), float32)\n",
    "\n",
    "Observation Space: Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m memory \u001b[38;5;241m=\u001b[39m ReplayMemory(CAPACITY)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# call the train_function, move everything below into callable functions\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(policy_net\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDQN_policy_net_Pendulum.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# look at what to save in the end\u001b[39;00m\n\u001b[1;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(target_net\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDQN_target_net_Pendulum.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# look at what to save in the end\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(env, num_episodes, policy_net, target_net, memory, optimizer, actions, device, eval_freq, cts)\u001b[0m\n\u001b[1;32m     36\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights, instead of hard update every X iterations\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# target_net_state_dict = target_net.state_dict()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#     target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# target_net.load_state_dict(target_net_state_dict)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# print(f\"Total reward: {total_reward}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 39\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(policy_net, target_net, optimizer, memory)\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(state_action_values, expected_state_action_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# loss = F.huber_loss(state_action_values, expected_state_action_values.unsqueeze(1))\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# loss = loss_function(state_action_values, expected_state_action_values.unsqueeze(1), reward_batch, 0, action_batch)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Optimize the model, i.e. gradient update\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Practicality dictates that we zero the gradients before backpropagation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# In-place gradient clipping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforce/lib/python3.12/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforce/lib/python3.12/site-packages/torch/_dynamo/decorators.py:46\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     44\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforce/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:454\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    451\u001b[0m on_enter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_enter\n\u001b[1;32m    452\u001b[0m backend_ctx_ctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_ctx_ctor\n\u001b[0;32m--> 454\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@functools\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwraps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m_fn\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDisableContext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_symbolic_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_fx_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_on_nested_fx_trace\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/reinforce/lib/python3.12/functools.py:35\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[0;34m(wrapper, wrapped, assigned, updated)\u001b[0m\n\u001b[1;32m     32\u001b[0m WRAPPER_ASSIGNMENTS \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__module__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__type_params__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m WRAPPER_UPDATES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m,)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_wrapper\u001b[39m(wrapper,\n\u001b[1;32m     36\u001b[0m                    wrapped,\n\u001b[1;32m     37\u001b[0m                    assigned \u001b[38;5;241m=\u001b[39m WRAPPER_ASSIGNMENTS,\n\u001b[1;32m     38\u001b[0m                    updated \u001b[38;5;241m=\u001b[39m WRAPPER_UPDATES):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update a wrapper function to look like the wrapped function\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m       wrapper is the function to be updated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m       function (defaults to functools.WRAPPER_UPDATES)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m assigned:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "rewards_list = []\n",
    "\n",
    "eps_decay = 10000 # Pendulum needs less exploration\n",
    "eps_start = 0.9 # Epsilon Greedy with decay rate based on steps taken for exploration\n",
    "eps_end = 0.001\n",
    "\n",
    "num_episodes = 10000 # number of training iterations\n",
    "eval_freq = 200 # sumarize trained episodes frequency\n",
    "\n",
    "# Discretized Action Space\n",
    "action_range = (env.action_space.low[0], env.action_space.high[0])\n",
    "n_actions = 31\n",
    "actions = np.linspace(action_range[0], action_range[1], n_actions)\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "# call the train_function, move everything below into callable functions\n",
    "\n",
    "train_model(env, num_episodes, policy_net, target_net, memory, optimizer, actions, device, eval_freq, cts=True)\n",
    "torch.save(policy_net.state_dict(), f\"DQN_policy_net_Pendulum.pt\") # look at what to save in the end\n",
    "torch.save(target_net.state_dict(), f\"DQN_target_net_Pendulum.pt\") # look at what to save in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target net\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "\n",
    "# Discretized Action Space\n",
    "action_range = (env.action_space.low[0], env.action_space.high[0])\n",
    "n_actions = 10\n",
    "actions = np.linspace(action_range[0], action_range[1], n_actions)\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(torch.load(\"DQN_policy_net_Pendulum.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Plotting for non-Linux-nerds\n",
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "# run trained model on environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in count():\n",
    "    action = select_action(env, target_net, state, actions, cts=True, explore=False)\n",
    "    observation, reward, terminated, truncated, _ = env.step([actions[action.item()]])\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "all_reward = []\n",
    "for i in range(2000):\n",
    "    # run trained model and print average reward over the 5 episodes\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(env, target_net, state, actions, cts=True, explore=False)\n",
    "        observation, reward, terminated, truncated, _ = env.step([actions[action.item()]])\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            all_reward.append(total_reward)\n",
    "            break\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    # print(f\"Episode {i+1} Reward: {total_reward}\")\n",
    "env.close()\n",
    "print(f\"Average Reward: {sum(all_reward)/len(all_reward)}\")\n",
    "print(f\"Max reward: {max(all_reward)}, Min reward: {min(all_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
